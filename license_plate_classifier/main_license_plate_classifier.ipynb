{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>License Plate Classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data_Preprocessing</h2>\n",
    "This segment encodes the license plate data from (https://github.com/datanews/license-plates) and save the dataframe as a pickle (serialized) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date   plate  label\n",
      "0  2010-10-01  ANDARE      1\n",
      "1  2010-10-01   1TWIN      1\n",
      "2  2010-10-01  11VROD      1\n",
      "3  2010-10-01  4SKNMC      1\n",
      "4  2010-10-01  7IRON6      1\n",
      "1    131989\n",
      "0      1646\n",
      "Name: label, dtype: int64\n",
      "length of dataset: 133635\n",
      "length of dataset: 1336\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "# import files\n",
    "accepted = pd.read_csv('accepted-plates.csv')\n",
    "rejected = pd.read_csv('rejected-plates.csv')\n",
    "\n",
    "# create dataset\n",
    "accepted['label'] = 1\n",
    "rejected['label'] = 0\n",
    "dataset = pd.concat([accepted, rejected], ignore_index=True)\n",
    "\n",
    "# filter out nan values\n",
    "dataset = dataset[dataset['plate'].notna()]\n",
    "\n",
    "print(dataset.head())\n",
    "\n",
    "# print the number of accepted and rejected plates\n",
    "print(dataset['label'].value_counts())\n",
    "\n",
    "print(f'length of dataset: {len(dataset)}')\n",
    "# make the dataset smaller\n",
    "dataset = dataset.sample(frac=0.01, random_state=1)\n",
    "print(f'length of dataset: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numba import cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "# Initialize the GPU\n",
    "# cuda.select_device(0)\n",
    "\n",
    "# use BERT to encode the text characters\n",
    "def encode_text(text):\n",
    "    # encode the text characters (not words)\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    # get the hidden states from the model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]\n",
    "    # get the first token embedding\n",
    "    features = last_hidden_states[:, 0, :].numpy()\n",
    "    return features\n",
    "\n",
    "# split dataset into train and test\n",
    "X = dataset['plate']\n",
    "y = dataset['label']\n",
    "\n",
    "# encode the text\n",
    "encoded = X.apply(encode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numba import cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "'''\n",
    "# Initialize the GPU\n",
    "cuda.select_device(0)\n",
    "\n",
    "# use BERT to encode the text characters\n",
    "@cuda.jit\n",
    "def process_features(features, output):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < features.shape[0]:\n",
    "        # process the features here as needed\n",
    "        output[pos] = features[pos]\n",
    "\n",
    "# split dataset into train and test\n",
    "X = dataset['plate'].values  # convert to Numpy array\n",
    "y = dataset['label'].values  # convert to Numpy array\n",
    "\n",
    "# encode the text on the CPU\n",
    "encoded = np.zeros((len(X), 768))  # assuming the feature size is 768\n",
    "for i, text in enumerate(X):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]\n",
    "    encoded[i] = last_hidden_states[:, 0, :].numpy()\n",
    "\n",
    "# process the encoded data on the GPU\n",
    "output = np.zeros_like(encoded)\n",
    "threadsperblock = 32\n",
    "blockspergrid = (len(encoded) + (threadsperblock - 1)) // threadsperblock\n",
    "process_features[blockspergrid, threadsperblock](encoded, output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded shape is (1336,)\n",
      "             date     plate  label  \\\n",
      "94621  2013-06-22    I970SS      1   \n",
      "63814  2012-06-02    PD4419      1   \n",
      "30496  2011-06-23    173INF      1   \n",
      "38111  2011-09-07  IAMA46ER      1   \n",
      "59102  2012-04-19  SUPES429      1   \n",
      "\n",
      "                                           encoded_plate  \n",
      "94621  [[-0.6183268, 0.21730568, 0.17258556, -0.12839...  \n",
      "63814  [[-0.9495514, -0.113612294, -0.21341404, -0.31...  \n",
      "30496  [[-0.51866204, -0.13206097, 0.036276437, -0.32...  \n",
      "38111  [[-0.77778316, -0.049498416, -0.15912797, -0.0...  \n",
      "59102  [[-0.58588433, 0.053693242, -0.10338338, -0.14...  \n"
     ]
    }
   ],
   "source": [
    "print(f'encoded shape is {encoded.shape}')\n",
    "\n",
    "# filter the encoded dataframe to only include the rows that are in the dataset dataframe\n",
    "encoded_filtered = encoded.loc[dataset.index]\n",
    "\n",
    "# reshape the encoded values to 1D array\n",
    "encoded_plate = encoded_filtered.values.reshape(-1)\n",
    "\n",
    "# add the encoded plate to the dataset\n",
    "dataset['encoded_plate'] = encoded_plate\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the dataset as to not have to re-encode the text\n",
    "import pickle\n",
    "\n",
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train set distribution after:  (array([0, 1]), array([  18, 1050]))\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "# train the model\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# since there are more rejected plates than accepted, we need to balance the dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ros = RandomOverSampler(random_state=0)\n",
    "# X_resampled, y_resampled = ros.fit_resample(encoded.to_numpy().reshape(-1, 1), y)\n",
    "\n",
    "# split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(encoded, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# print the distribution of the labels\n",
    "print('\\n\\nTrain set distribution after: ', np.unique(y_train, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1068,)\n",
      "y_train shape: (1068,)\n",
      "encoded shape: (1336,)\n",
      "accuracy: 0.9925373134328358\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "\n",
    "print(f'encoded shape: {encoded.shape}')\n",
    "\n",
    "# train the model using a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "\n",
    "model.fit(np.vstack(X_train), y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(np.vstack(X_train))\n",
    "y_pred = model.predict(np.vstack(X_test))\n",
    "\n",
    "# print the accuracy\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925373134328358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      1.00       266\n",
      "\n",
      "    accuracy                           0.99       268\n",
      "   macro avg       0.50      0.50      0.50       268\n",
      "weighted avg       0.99      0.99      0.99       268\n",
      "\n",
      "[[  0   2]\n",
      " [  0 266]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# check accuracy and f1 score and confusion matrix\n",
    "print(accuracy_score(y_test.tolist(), y_pred))\n",
    "print(classification_report(y_test.tolist(), y_pred))\n",
    "print(confusion_matrix(y_test.tolist(), y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
